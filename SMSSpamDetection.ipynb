{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlpnOIDa+PdMvOVlwobS2H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report\n","from nltk.tokenize import word_tokenize\n","\n","# Load the SMS Spam Collection Dataset with specified encoding\n","df = pd.read_csv(\"spam.csv\", encoding='latin-1')\n","\n","\n","# Preprocessing\n","# Convert labels to numerical values (0 for ham, 1 for spam)\n","encoder = LabelEncoder()\n","df['label'] = encoder.fit_transform(df['v1'])\n","\n","# Tokenization\n","df['tokenized_text'] = df['v2'].apply(word_tokenize)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(df['tokenized_text'], df['label'], test_size=0.2, random_state=42)\n","\n","# Vectorization\n","vectorizer = CountVectorizer()\n","X_train_vec = vectorizer.fit_transform([' '.join(tokens) for tokens in X_train])\n","X_test_vec = vectorizer.transform([' '.join(tokens) for tokens in X_test])\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train_vec.toarray(), dtype=torch.float32)\n","y_train_torch = torch.tensor(y_train.values, dtype=torch.long)\n","X_test_torch = torch.tensor(X_test_vec.toarray(), dtype=torch.float32)\n","y_test_torch = torch.tensor(y_test.values, dtype=torch.long)\n","\n","# Define the model architecture\n","class SpamClassifier(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(SpamClassifier, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# Instantiate the model, loss function, and optimizer\n","input_size = X_train_torch.shape[1]\n","hidden_size = 128\n","num_classes = 2\n","model = SpamClassifier(input_size, hidden_size, num_classes)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","batch_size = 32\n","train_dataset = [(X_train_torch[i], y_train_torch[i]) for i in range(len(X_train_torch))]\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","for epoch in range(num_epochs):\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# Evaluation\n","with torch.no_grad():\n","    outputs = model(X_test_torch)\n","    _, predicted = torch.max(outputs, 1)\n","    accuracy = accuracy_score(y_test_torch.numpy(), predicted.numpy())\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(classification_report(y_test_torch.numpy(), predicted.numpy()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxYB_k89l4Xv","executionInfo":{"status":"ok","timestamp":1710464048938,"user_tz":240,"elapsed":39652,"user":{"displayName":"sreeja k","userId":"02746799458689879032"}},"outputId":"4f0172b2-1aec-4bc9-ef0f-eaabc64c33d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.0757\n","Epoch [2/10], Loss: 0.0157\n","Epoch [3/10], Loss: 0.0053\n","Epoch [4/10], Loss: 0.0031\n","Epoch [5/10], Loss: 0.0007\n","Epoch [6/10], Loss: 0.0020\n","Epoch [7/10], Loss: 0.0003\n","Epoch [8/10], Loss: 0.0000\n","Epoch [9/10], Loss: 0.0001\n","Epoch [10/10], Loss: 0.0000\n","Accuracy: 0.9794\n","              precision    recall  f1-score   support\n","\n","           0       0.98      1.00      0.99       965\n","           1       0.98      0.86      0.92       150\n","\n","    accuracy                           0.98      1115\n","   macro avg       0.98      0.93      0.95      1115\n","weighted avg       0.98      0.98      0.98      1115\n","\n"]}]}]}
